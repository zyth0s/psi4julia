{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Manipulation: Psi4 and NumPy manipulation routines\n",
    "Contracting tensors together forms the core of the Psi4Julia project. First let us consider the popluar [Einstein Summation Notation](https://en.wikipedia.org/wiki/Einstein_notation) which allows for very succinct descriptions of a given tensor contraction.\n",
    "\n",
    "For example, let us consider a [inner (dot) product](https://en.wikipedia.org/wiki/Dot_product):\n",
    "$$c = \\sum_{ij} A_{ij} * B_{ij}$$\n",
    "\n",
    "With the Einstein convention, all indices that are repeated are considered summed over, and the explicit summation symbol is dropped:\n",
    "$$c = A_{ij} * B_{ij}$$\n",
    "\n",
    "This can be extended to [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication):\n",
    "\\begin{align}\n",
    "\\rm{Conventional}\\;\\;\\;  C_{ik} &= \\sum_{j} A_{ij} * B_{jk} \\\\\n",
    "\\rm{Einstein}\\;\\;\\;  C &= A_{ij} * B_{jk} \\\\\n",
    "\\end{align}\n",
    "\n",
    "Where the $C$ matrix has *implied* indices of $C_{ik}$ as the only repeated index is $j$.\n",
    "\n",
    "However, there are many cases where this notation fails. Thus we often use the generalized Einstein convention. To demonstrate let us examine a [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)):\n",
    "$$C_{ij} = \\sum_{ij} A_{ij} * B_{ij}$$\n",
    "\n",
    "\n",
    "This operation is nearly identical to the dot product above, and is not able to be written in pure Einstein convention. The generalized convention allows for the use of indices on the left hand side of the equation:\n",
    "$$C_{ij} = A_{ij} * B_{ij}$$\n",
    "\n",
    "Usually it should be apparent within the context the exact meaning of a given expression.\n",
    "\n",
    "Finally we also make use of Matrix notation:\n",
    "\\begin{align}\n",
    "{\\rm Matrix}\\;\\;\\;  \\bf{D} &= \\bf{A B C} \\\\\n",
    "{\\rm Einstein}\\;\\;\\;  D_{il} &= A_{ij} B_{jk} C_{kl}\n",
    "\\end{align}\n",
    "\n",
    "Note that this notation is signified by the use of bold characters to denote matrices and consecutive matrices next to each other imply a chain of matrix multiplications! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einsum\n",
    "\n",
    "To perform most operations we turn to tensor packages (here we use [NumPy's einsum function](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html) but there are similar Julia packages, Einsum.jl, Tullio.jl, ..., that might be explored in future revisions). Those allow Einsten convention as an input. In addition to being much easier to read, manipulate, and change, it is usually more efficient than our loop implementation.\n",
    "\n",
    "To begin let us consider the construction of the following tensor (which you may recognize):\n",
    "$$G_{pq} = 2.0 * I_{pqrs} D_{rs} - 1.0 * I_{prqs} D_{rs}$$ \n",
    "\n",
    "First let us import our normal suite of modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <module 'numpy' from '/Users/daniel/miniconda3/envs/p4env/lib/python3.7/site-packages/numpy/__init__.py'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using PyCall: pyimport\n",
    "psi4 = pyimport(\"psi4\")\n",
    "np   = pyimport(\"numpy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use conventional Julia loops and einsum to perform the same task. Keep size relatively small as these 4-index tensors grow very quickly in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools: @btime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `@btime` we measure execution time several times to have more reliable timings than with `@time` (single execution)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**WARNING: We are using Julia's global variables, and those are known to be less efficient than local variables. It is better to wrap code inside function. For large computations we do a single execution so the timings will also include compilation time. Succesive runs are faster.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for loop G build:\n",
      "  81.608 ms (2400038 allocations: 61.04 MiB)\n",
      "Time for einsum G build:\n",
      "  572.990 μs (140 allocations: 20.16 KiB)\n",
      "Loop and einsum builds of the Fock matrix match?    true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dims = 20\n",
    "\n",
    "@assert dims <= 30 \"Size must be smaller than 30.\"\n",
    "D = rand(dims, dims)\n",
    "I = rand(dims, dims, dims, dims)\n",
    "\n",
    "# Build the Fock matrix using loops, while keeping track of time\n",
    "println(\"Time for loop G build:\")\n",
    "Gloop = @btime begin\n",
    "   Gloop = np.zeros((dims, dims))\n",
    "   @inbounds for ind in CartesianIndices(I)\n",
    "       p, q, r, s = Tuple(ind)\n",
    "       Gloop[p, q] += 2I[p, q, r, s] * D[r, s]\n",
    "       Gloop[p, q] -=  I[p, r, q, s] * D[r, s]\n",
    "   end\n",
    "   Gloop\n",
    "end\n",
    "\n",
    "# Build the Fock matrix using einsum, while keeping track of time\n",
    "println(\"Time for einsum G build:\")\n",
    "G = @btime begin\n",
    "   J = np.einsum(\"pqrs,rs\", I, D, optimize=true)\n",
    "   K = np.einsum(\"prqs,rs\", I, D, optimize=true)\n",
    "   G = 2J - K\n",
    "end\n",
    "\n",
    "# Make sure the correct answer is obtained\n",
    "println(\"Loop and einsum builds of the Fock matrix match?    \", np.allclose(G, Gloop))\n",
    "println()\n",
    "# Print out relative times for explicit loop vs einsum Fock builds\n",
    "#println(\"G builds with einsum are $(g_loop_time/einsum_time) times faster than Julia loops!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the einsum function can be considerably faster than the plain Julia loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix multiplication chain/train\n",
    "\n",
    "Now let us turn our attention to a more canonical matrix multiplication example such as:\n",
    "$$D_{il} = A_{ij} B_{jk} C_{kl}$$\n",
    "\n",
    "We could perform this operation using einsum; however, matrix multiplication is an extremely common operation in all branches of linear algebra. Thus, these functions have been optimized to be more efficient than the `einsum` function. The matrix product will explicitly compute the following operation:\n",
    "$$C_{ij} = A_{ij} * B_{ij}$$\n",
    "\n",
    "This is Julia's matrix multiplication method `*`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair product allclose? true\n"
     ]
    }
   ],
   "source": [
    "dims = 200\n",
    "A = rand(dims, dims)\n",
    "B = rand(dims, dims)\n",
    "C = rand(dims, dims)\n",
    "\n",
    "# First compute the pair product\n",
    "tmp_dot = A * B\n",
    "tmp_einsum = np.einsum(\"ij,jk->ik\", A, B, optimize=true)\n",
    "println(\"Pair product allclose? \", np.allclose(tmp_dot, tmp_einsum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have proved exactly what the dot product does, let us consider the full chain and do a timing comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain multiplication allclose? true\n",
      "\n",
      "* time:\n",
      "  470.373 μs (4 allocations: 625.16 KiB)\n",
      "np.einsum time\n",
      "  1.498 s (52 allocations: 315.25 KiB)\n"
     ]
    }
   ],
   "source": [
    "D_dot = A * B * C\n",
    "D_einsum = np.einsum(\"ij,jk,kl->il\", A, B, C, optimize=true)\n",
    "println(\"Chain multiplication allclose? \", np.allclose(D_dot, D_einsum))\n",
    "\n",
    "println()\n",
    "println(\"* time:\")\n",
    "@btime A * B * C\n",
    "\n",
    "println(\"np.einsum time\")\n",
    "# no optimization here for illustrative purposes!\n",
    "@btime np.einsum(\"ij,jk,kl->il\", A, B, C);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On most machines the `*` times are roughly ~2,000 times faster. The reason is twofold:\n",
    " - The `*` routines typically call [Basic Linear Algebra Subprograms (BLAS)](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms). The BLAS routines are highly optimized and threaded versions of the code.\n",
    " - The `np.einsum` code will not factorize the operation by default; Thus, the overall cost is ${\\cal O}(N^4)$ (as there are four indices) rather than the factored $(\\bf{A B}) \\bf{C}$ which runs ${\\cal O}(N^3)$.\n",
    " \n",
    "The first issue is difficult to overcome; however, the second issue can be resolved by the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.einsum factorized time:\n",
      "  8.198 ms (96 allocations: 629.84 KiB)\n"
     ]
    }
   ],
   "source": [
    "println(\"np.einsum factorized time:\")\n",
    "# no optimization here for illustrative purposes!\n",
    "@btime np.einsum(\"ik,kl->il\", np.einsum(\"ij,jk->ik\", A, B), C);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On most machines the factorized `einsum` expression is only ~10 times slower than `*`. While a massive improvement, this is a clear demonstration the BLAS usage is usually recommended. Thankfully, in Julia its syntax is very clear. The Psi4Julia project tends to lean toward usage of tensor pacakges but if Julia's built-in matrix multiplication is faster we would use it.\n",
    "\n",
    "Starting in NumPy 1.12, the [einsum function](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html) has a `optimize` flag which will automatically factorize the einsum code for you using a greedy algorithm, leading to considerable speedups at almost no cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.einsum optimized time\n",
      "  942.849 μs (74 allocations: 316.47 KiB)\n"
     ]
    }
   ],
   "source": [
    "println(\"np.einsum optimized time\")\n",
    "@btime np.einsum(\"ij,jk,kl->il\", A, B, C, optimize=true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, using `optimize=true` for automatic factorization is \"only\" 50% slower than `*`. Furthermore, it is ~8 times faster than factorizing the expression by hand, which represents a very good trade-off between speed and readability. When unsure, `optimize=true` is strongly recommended. The real value of tensor packages will become tangible for more complicated expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complicated tensor manipulations\n",
    "Let us consider a popular index transformation example:\n",
    "$$M_{pqrs} = C_{pi} C_{qj} I_{ijkl} C_{rk} C_{sl}$$\n",
    "\n",
    "Here, a naive `einsum` call would scale like $\\mathcal{O}(N^8)$ which translates to an extremely costly computation for all but the smallest $N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting np.einsum N^8 transformation...complete in 18.906630951 s\n",
      "\n",
      "Starting np.einsum N^5 transformation with einsum ... complete in 0.005507732 s \n",
      "N^5 is 3432.743450661724 faster than N^8 algorithm!\n",
      "Allclose? true\n",
      "\n",
      "Now np.einsum optimized transformation... complete in 0.001266589 s \n",
      "\n",
      "Starting Julia's N^5 transformation with * ... complete in 0.001528347 s \n",
      "Allclose? true\n",
      "N^5 is 12370.640274100057 faster than N^8 algorithm!\n"
     ]
    }
   ],
   "source": [
    "# Grab orbitals\n",
    "dims = 15\n",
    "@assert dims <= 15 || \"Size must be smaller than 15.\"\n",
    "    \n",
    "C = rand(dims, dims)\n",
    "I = rand(dims, dims, dims, dims)\n",
    "\n",
    "# Numpy's einsum N^8 transformation.\n",
    "print(\"\\nStarting np.einsum N^8 transformation...\")\n",
    "# no optimization here for illustrative purposes!\n",
    "n8_time = @elapsed MO_n8 = np.einsum(\"pI,qJ,pqrs,rK,sL->IJKL\", C, C, I, C, C)\n",
    "print(\"complete in $n8_time s\\n\")\n",
    "\n",
    "# Numpy's einsum N^5 transformation.\n",
    "print(\"\\nStarting np.einsum N^5 transformation with einsum ... \")\n",
    "n5_time = @elapsed begin\n",
    "   # no optimization here for illustrative purposes!\n",
    "   MO_n5 = np.einsum(\"pA,pqrs->Aqrs\", C, I)\n",
    "   MO_n5 = np.einsum(\"qB,Aqrs->ABrs\", C, MO_n5)\n",
    "   MO_n5 = np.einsum(\"rC,ABrs->ABCs\", C, MO_n5)\n",
    "   MO_n5 = np.einsum(\"sD,ABCs->ABCD\", C, MO_n5)\n",
    "end\n",
    "print(\"complete in $n5_time s \\n\")\n",
    "println(\"N^5 is $(n8_time/n5_time) faster than N^8 algorithm!\")\n",
    "println(\"Allclose? \", np.allclose(MO_n8, MO_n5))\n",
    "\n",
    "# Numpy's einsum optimized transformation.\n",
    "print(\"\\nNow np.einsum optimized transformation... \")\n",
    "n8_time_opt = @elapsed MO_n8 = np.einsum(\"pI,qJ,pqrs,rK,sL->IJKL\", C, C, I, C, C, optimize=true)\n",
    "print(\"complete in $n8_time_opt s \\n\")\n",
    "\n",
    "# Julia's GEMM N^5 transformation.\n",
    "# Try to figure this one out!\n",
    "print(\"\\nStarting Julia's N^5 transformation with * ... \")\n",
    "dgemm_time = @elapsed begin\n",
    "   MO = C' * reshape(I, dims, :)\n",
    "   MO = reshape(MO, :, dims) * C\n",
    "   MO = permutedims(reshape(MO, dims, dims, dims, dims), (2, 1, 4, 3))\n",
    "\n",
    "   MO = C' * reshape(MO, dims, :)\n",
    "   MO = reshape(MO, :, dims) * C\n",
    "   MO = permutedims(reshape(MO, dims, dims, dims, dims),(2, 1, 4, 3))\n",
    "end\n",
    "print(\"complete in $dgemm_time s \\n\")\n",
    "println(\"Allclose? \", np.allclose(MO_n8, MO))\n",
    "println(\"N^5 is $(n8_time/dgemm_time) faster than N^8 algorithm!\")\n",
    "\n",
    "# There are still several possibilities to explore:\n",
    "# @inbounds, @simd, LinearAlgebra.LAPACK calls, Einsum.jl, Tullio.jl, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "jl:light,ipynb"
  },
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
